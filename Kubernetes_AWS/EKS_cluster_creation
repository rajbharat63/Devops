login to the AWS portal.
Open the cloud shell from the icon on the top.
Navigate to https://eksctl.io/installation/ to install eksctl command line. kubectl and aws cli comes by default.
kubectl version --> gets the version of kubectl 
aws --version --> gives the version of aws cli

Creating kubernetes cluster using EKS (managed aws cluster), this will create the master node in multiple AZ with HA.
eksctl create cluster --name=eksdemo-raj1 --without-nodegroup               (Nodegroup = worker node, we can specify 1 node group or multiple)

Cloud Formation service will be used to create the managed kubernetes cluster.
Search for cloud formation and gives us the RESOURCES which are getting created to fullfil the requirements. TEMPLATE will show us the resources and AWS zones in the yaml format.
Once the K8 stack gets created. Navigate to EKS (enter this in the search bar) this will provide us the cluster which was created.
eksdemo-raj1	Active	1.29

To create the worker node or any other resource, please use this command.
eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster eksdemo-raj1 --approve                     --> provide the right region name and cluster name

Creating NodeGroup (Worker Node) with the help of following command.
KeyPair should be under EC2 instance service --> keypair and if we do not have the keypair created using "ssh-keygen -t rsa -b 2048 -f ~/.ssh/my-aws-key" to generate the key 
Navigate to EC2 service --> Network abd Security --> Keypair --> import the key which was generated and run the below command to create a nodegroup

eksctl create nodegroup --cluster=eksdemo-raj1 --name=eksdemo-raj1-ng --node-type=m5.large --nodes=2 --nodes-min=2 --nodes-max=4 --node-volume-size=20 --ssh-access --ssh-public-key=my-aws-access-key --managed --asg-access --external-dns-access --full-ecr-access --appmesh-access --alb-ingress-access


Under Cloud Formation --> a new stack gets created "eksctl-eksdemo-raj1-nodegroup-eksdemo-raj1-ng" --> if you go inside it provides you the resources required the creation of nodegroup.

Under EKS service --> go into the master cluster --> Compute --> gives you the worker nodes which got created.


#kubectl get nodes -o wide--> gives you the worker nodes which was created. this command shows wide options with the IP, IMAGE, Container RUNTIME


____________________

Creation of pods

applying the deployment.yaml file

kubectl apply -f deployment.yaml
[cloudshell-user@ip-10-140-102-84 ~]$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE                            NOMINATED NODE   READINESS GATES
nginx-deploy-5bdd559bcc-5btpw   1/1     Running   0          10m   192.168.8.68     ip-192-168-27-78.ec2.internal   <none>           <none>
nginx-deploy-5bdd559bcc-cgvpz   1/1     Running   0          10m   192.168.55.93    ip-192-168-42-51.ec2.internal   <none>           <none>
nginx-deploy-5bdd559bcc-hlbwm   1/1     Running   0          10m   192.168.61.199   ip-192-168-42-51.ec2.internal   <none>           <none>


[cloudshell-user@ip-10-140-102-84 ~]$ kubectl delete -f deployment.yaml 
deployment.apps "nginx-deploy" deleted

[cloudshell-user@ip-10-140-102-84 ~]$ kubectl get pods -o wide
No resources found in default namespace.


_________________________


Deleting the EKS cluster which was created ----------> Deletion can be done via CloudFormation service in AWS

eksctl get cluster

eksctl get nodegroup

eksctl delete nodegroup --cluster=eksdemo-raj1 --name=eksdemo-raj1-ng

eksctl delete cluster eksdemo-raj1

___________________________




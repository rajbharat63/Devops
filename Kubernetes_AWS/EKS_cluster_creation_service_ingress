login to the AWS portal.
Open the cloud shell from the icon on the top.
Navigate to https://eksctl.io/installation/ to install eksctl command line. kubectl and aws cli comes by default.
kubectl version --> gets the version of kubectl 
aws --version --> gives the version of aws cli

Creating kubernetes cluster using EKS (managed aws cluster), this will create the master node in multiple AZ with HA.
eksctl create cluster --name=eksdemo-raj1 --without-nodegroup               (Nodegroup = worker node, we can specify 1 node group or multiple)

Cloud Formation service will be used to create the managed kubernetes cluster.
Search for cloud formation and gives us the RESOURCES which are getting created to fullfil the requirements. TEMPLATE will show us the resources and AWS zones in the yaml format.
Once the K8 stack gets created. Navigate to EKS (enter this in the search bar) this will provide us the cluster which was created.
eksdemo-raj1	Active	1.29

To create the worker node or any other resource, please use this command.
eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster eksdemo-raj1 --approve                     --> provide the right region name and cluster name

Creating NodeGroup (Worker Node) with the help of following command.
KeyPair should be under EC2 instance service --> keypair and if we do not have the keypair created using "ssh-keygen -t rsa -b 2048 -f ~/.ssh/my-aws-key" to generate the key 
Navigate to EC2 service --> Network abd Security --> Keypair --> import the key which was generated and run the below command to create a nodegroup

eksctl create nodegroup --cluster=eksdemo-raj1 --name=eksdemo-raj1-ng --node-type=m5.large --nodes=2 --nodes-min=2 --nodes-max=4 --node-volume-size=20 --ssh-access --ssh-public-key=my-aws-access-key --managed --asg-access --external-dns-access --full-ecr-access --appmesh-access --alb-ingress-access


Under Cloud Formation --> a new stack gets created "eksctl-eksdemo-raj1-nodegroup-eksdemo-raj1-ng" --> if you go inside it provides you the resources required the creation of nodegroup.

Under EKS service --> go into the master cluster --> Compute --> gives you the worker nodes which got created.


#kubectl get nodes -o wide--> gives you the worker nodes which was created. this command shows wide options with the IP, IMAGE, Container RUNTIME


____________________

Creation of pods

applying the deployment.yaml file

kubectl apply -f deployment.yaml
[cloudshell-user@ip-10-140-102-84 ~]$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE                            NOMINATED NODE   READINESS GATES
nginx-deploy-5bdd559bcc-5btpw   1/1     Running   0          10m   192.168.8.68     ip-192-168-27-78.ec2.internal   <none>           <none>
nginx-deploy-5bdd559bcc-cgvpz   1/1     Running   0          10m   192.168.55.93    ip-192-168-42-51.ec2.internal   <none>           <none>
nginx-deploy-5bdd559bcc-hlbwm   1/1     Running   0          10m   192.168.61.199   ip-192-168-42-51.ec2.internal   <none>           <none>


[cloudshell-user@ip-10-140-102-84 ~]$ kubectl delete -f deployment.yaml 
deployment.apps "nginx-deploy" deleted

[cloudshell-user@ip-10-140-102-84 ~]$ kubectl get pods -o wide
No resources found in default namespace.


_________________________


Deleting the EKS cluster which was created ----------> Deletion can be done via CloudFormation service in AWS

eksctl get cluster

eksctl get nodegroup

eksctl delete nodegroup --cluster=eksdemo-raj1 --name=eksdemo-raj1-ng

eksctl delete cluster eksdemo-raj1

___________________________


Ingress --> Path or URL based routing.

Node A --> 3 pods of application/Microservice A and 3 pods of application/Microservice B

Ingress will route the traffic to the microservice or application or pod/container based on the path or URL based routing. Each individual service will be defined for application A and B.

Ingress has 2 components

1. Ingress Controller --> Deployed this in the cluster to read the ingress resource and its rules. This is kind of Load Balancer. Example: nfinx ingress controller, AWS ALB etc
2. Ingress Resource --> Rules to re-direcct the request to the services


Working:

1. Install nignx ingress controller in the cluster and this will install AWS Elastic LB in the cloud.
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/cloud/deploy.yaml

After installing nginx ingress controller --> NS, SVC, Roles, Rolebindings, pods etc are created as part of above yaml.

2. This will create a AWS LB --> Go to EC2 --> LoadBalancer

3. From the YAML_files folder application1.yaml will deploy 2 replicas of pods and a service which routes the traffic towards the pods. If we not mention which type of service such as NordPort or LB or ClusterIP, by default this will create a ClusterIP service.

kubectl apply -f application1.yaml

[cloudshell-user@ip-10-138-181-71 ~]$ kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/nginx-deploy-7d69d6b977-272mx   1/1     Running   0          20s
pod/nginx-deploy-7d69d6b977-glbl7   1/1     Running   0          20s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1      <none>        443/TCP   125m
service/nginx-svc    ClusterIP   10.100.164.21   <none>        80/TCP    20s

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-deploy   2/2     2            2           20s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-deploy-7d69d6b977   2         2         2       20s



After creation of ClusterIP service if we try to connect to the application it will not be routed as the rules were not defined yet.

[cloudshell-user@ip-10-138-181-71 ~]$ kubectl get svc -n ingress-nginx
NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.100.239.237   a2aa0fd503c0b45b8ad631f530afcfd3-878639005.us-east-1.elb.amazonaws.com   80:31963/TCP,443:31171/TCP   16m
ingress-nginx-controller-admission   ClusterIP      10.100.128.56    <none>                       443/TCP     

Apply the ingress rule

[cloudshell-user@ip-10-138-181-71 ~]$ kubectl apply -f ingress_rules.yaml 
ingress.networking.k8s.io/ingress-nginx created

With a2aa0fd503c0b45b8ad631f530afcfd3-878639005.us-east-1.elb.amazonaws.com/test we will reach ingress-nginx controller (controller got created in ingress-nginx ns) service (nginx-svc in the default ns) -->>>> pods were deployed with a map to the nginx-svc on default ns.

****Deletion in the reverse order, this is costly ***

kubectl delete -f ingress_rules.yaml 
kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/cloud/deploy.yaml
kubectl delete -f application1.yaml 
eksctl delete nodegroup --cluster=eksdemo-raj1 --name=eksdemo-raj1-ng
eksctl delete cluster eksdemo-raj1






